{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jmspa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "#visual\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "\n",
    "#nlp\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import torch\n",
    "import emoji\n",
    "from googletrans import Translator\n",
    "\n",
    "# from google.colab import files\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset, random_split\n",
    "from tqdm import tqdm\n",
    "import statistics\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "combined_df = pd.read_excel('fix_fixed.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Label</th>\n",
       "      <th>translated</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>review_text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bangka blm bukan kantor pos ambil nya</td>\n",
       "      <td>1</td>\n",
       "      <td>bangka blm bukan kantor pos ambil nya</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bangka belum bukan kantor pos ambil nya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>halo min apa kpr sudah berjalan bisa minta di...</td>\n",
       "      <td>2</td>\n",
       "      <td>lingkaran cahaya min apa kpr sudah berjalan b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lingkaran cahaya min apa kpr sudah berjalan bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>perancis</td>\n",
       "      <td>1</td>\n",
       "      <td>perancis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>perancis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kapan giveaway nya nih megamin [smirking face]</td>\n",
       "      <td>0</td>\n",
       "      <td>kapan memberi secara gratis nya nih megamin w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kapan memberi secara gratis nya ini megamin wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>geh yang balam tau gak soto tangkar yang dulu ...</td>\n",
       "      <td>1</td>\n",
       "      <td>geh yang balam tau gak soto tangkar yang dulu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>geh yang balam tahu tidak soto tangkar yang du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>kak brarti perlu download welma lagi kan utk b...</td>\n",
       "      <td>0</td>\n",
       "      <td>kak brarti perlu unduh welma lagi kan utk beli...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kak berarti perlu unduh welma lagi kan untuk b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>semoga kita terhindar dari modus penipuan bri ...</td>\n",
       "      <td>0</td>\n",
       "      <td>semoga kita terhindar dari modus penipuan bri ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>semoga kita terhindar dari modus penipuan bri ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>hah dimana tuh tahun kemaren ngajuin mintanya...</td>\n",
       "      <td>2</td>\n",
       "      <td>ha dimana tuh tahun kemaren ngajuin mintanya 2jt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tertawa di mana itu tahun kemarin ngajuin mint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>masang nomor mudah mudahan aja bisa tembus tuh...</td>\n",
       "      <td>0</td>\n",
       "      <td>masang nomor mudah mudahan aja bisa tembus tuh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>memasang nomor mudah mudahan saja bisa tembus ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>pengalaman buka rekening melalui marketingnya ...</td>\n",
       "      <td>2</td>\n",
       "      <td>pengalaman buka rekening melalui marketingnya ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pengalaman buka rekening melalui marketingnya ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31999 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Data  Label  \\\n",
       "0                  bangka blm bukan kantor pos ambil nya      1   \n",
       "1       halo min apa kpr sudah berjalan bisa minta di...      2   \n",
       "2                                              perancis       1   \n",
       "3        kapan giveaway nya nih megamin [smirking face]       0   \n",
       "4      geh yang balam tau gak soto tangkar yang dulu ...      1   \n",
       "...                                                  ...    ...   \n",
       "31994  kak brarti perlu download welma lagi kan utk b...      0   \n",
       "31995  semoga kita terhindar dari modus penipuan bri ...      0   \n",
       "31996   hah dimana tuh tahun kemaren ngajuin mintanya...      2   \n",
       "31997  masang nomor mudah mudahan aja bisa tembus tuh...      0   \n",
       "31998  pengalaman buka rekening melalui marketingnya ...      2   \n",
       "\n",
       "                                              translated Unnamed: 3  \\\n",
       "0                  bangka blm bukan kantor pos ambil nya        NaN   \n",
       "1       lingkaran cahaya min apa kpr sudah berjalan b...        NaN   \n",
       "2                                               perancis        NaN   \n",
       "3       kapan memberi secara gratis nya nih megamin w...        NaN   \n",
       "4       geh yang balam tau gak soto tangkar yang dulu...        NaN   \n",
       "...                                                  ...        ...   \n",
       "31994  kak brarti perlu unduh welma lagi kan utk beli...        NaN   \n",
       "31995  semoga kita terhindar dari modus penipuan bri ...        NaN   \n",
       "31996   ha dimana tuh tahun kemaren ngajuin mintanya 2jt        NaN   \n",
       "31997  masang nomor mudah mudahan aja bisa tembus tuh...        NaN   \n",
       "31998  pengalaman buka rekening melalui marketingnya ...        NaN   \n",
       "\n",
       "                                     review_text_cleaned  \n",
       "0                bangka belum bukan kantor pos ambil nya  \n",
       "1      lingkaran cahaya min apa kpr sudah berjalan bi...  \n",
       "2                                               perancis  \n",
       "3      kapan memberi secara gratis nya ini megamin wa...  \n",
       "4      geh yang balam tahu tidak soto tangkar yang du...  \n",
       "...                                                  ...  \n",
       "31994  kak berarti perlu unduh welma lagi kan untuk b...  \n",
       "31995  semoga kita terhindar dari modus penipuan bri ...  \n",
       "31996  tertawa di mana itu tahun kemarin ngajuin mint...  \n",
       "31997  memasang nomor mudah mudahan saja bisa tembus ...  \n",
       "31998  pengalaman buka rekening melalui marketingnya ...  \n",
       "\n",
       "[31999 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df['review_text_cleaned'] = combined_df['review_text_cleaned'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-large-p2\")\n",
    "config = BertConfig.from_pretrained(\"indobenchmark/indobert-large-p2\")\n",
    "config.num_labels = 3\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained(\"indobenchmark/indobert-large-p2\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length =  17.883246351448484\n",
      "Median length =  14\n",
      "Max lenght = 864\n",
      "Min lenght =  3\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "sent_length = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in combined_df['review_text_cleaned']:\n",
    "    # print(sent)\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    sent_length.append(len(input_ids))\n",
    "    if len(input_ids) == 1265:\n",
    "        print(sent)\n",
    "\n",
    "print('Average length = ', sum(sent_length)/len(sent_length))\n",
    "print('Median length = ', statistics.median(sent_length))\n",
    "print('Max lenght =', max(sent_length))\n",
    "print('Min lenght = ', min(sent_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\jmspa\\anaconda3\\envs\\coba_GRU\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 9180,  659,  ...,    0,    0,    0],\n",
      "        [   2, 7738, 3729,  ...,   41, 4189,    3],\n",
      "        [   2, 6057,    3,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2, 7519,   26,  ...,    0,    0,    0],\n",
      "        [   2, 5366, 1288,  ...,    0,    0,    0],\n",
      "        [   2, 1866, 3121,  ...,  629, 3588,    3]])\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])\n",
      "tensor([1, 2, 1,  ..., 2, 0, 2])\n",
      "Original:   bangka blm bukan kantor pos ambil nya\n",
      "Token IDs: tensor([   2, 9180,  659,  531, 1571,  598, 3422, 1107,    3,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all of the sentences and map the tokens to their word IDs.\n",
    "import torch\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# For every sentence...\n",
    "for sent in combined_df['review_text_cleaned']:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 24,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',# Return pytorch tensors.\n",
    "                   )\n",
    "\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert the lists into tensors.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(combined_df['Label'])\n",
    "\n",
    "print(input_ids)\n",
    "print(attention_masks)\n",
    "print(labels)\n",
    "# Print sentence 0, now as a list of IDs.\n",
    "print('Original: ', combined_df['translated'][0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)[0] # Models outputs are now tuples\n",
    "last_hidden_states = last_hidden_states.mean(1)\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X=last_hidden_states\n",
    "y=combined_df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6443661971830986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB,BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = last_hidden_states.numpy().reshape(-1, 1)\n",
    "\n",
    "num_bins = 10\n",
    "X_discretized = np.digitize(X, bins=np.linspace(X.min(), X.max(), num_bins))\n",
    "\n",
    "# Extract the labels from the DataFrame\n",
    "y = combined_df['Label'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_discretized, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier on the discretized embeddings\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2112676056338028\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = ComplementNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6443661971830986\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = BernoulliNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coba_GRU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
